{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3e11c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from rdflib import URIRef, BNode, Literal, Namespace\n",
    "from rdflib.namespace import DCTERMS, RDFS\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "# if we use embedding only from last layer, this should stay as it is\n",
    "# it could be changed for some experiments ?\n",
    "layers = [-1]\n",
    "\n",
    "#we load the model\n",
    "#we could experiment with other models as well\n",
    "model = AutoModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525e55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [json.load(open(x)) for x in glob.glob('./data/*.json')[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f88ec4",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "- <del> split the texts into sentences\n",
    "- <del> filter the sentences that have 2+ entities AND have a relations among these\n",
    "\n",
    "\n",
    "- baseline prediction (matching the blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e38740cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general functions\n",
    "#the device variable can be changed in case a GPU is available\n",
    "device = torch.device('cpu')\n",
    "#uncomment the next line to use gpu\n",
    "#device = torch.device('gpu')\n",
    "\n",
    "def split_sentences(sample):\n",
    "    sentence_boundaries = sample['sentences_boundaries']\n",
    "    sentences = []\n",
    "    text = sample[\"text\"]\n",
    "    for boundary in sentence_boundaries:\n",
    "        start= boundary[0]\n",
    "        end = boundary [1]\n",
    "        sentence = text[start:end]\n",
    "        sentences.append(sentence)\n",
    "    return sentences, sentence_boundaries\n",
    "\n",
    "def get_relations(sample):\n",
    "    sentence_list = []\n",
    "    sentences, sentence_boundaries = split_sentences(sample)\n",
    "    triples = sample['triples']\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_dict = {}\n",
    "        #it looks like some entities do not have boundaries, this would make it difficult to retrieve the tokens\n",
    "        #let's just not include them for now\n",
    "        triples_to_get = [x for x in triples if x['sentence_id'] == i and x['object']['boundaries'] != None and x['subject']['boundaries'] != None]\n",
    "        if len(triples_to_get) >= 1:\n",
    "            sentence_dict['sentence'] = sentence\n",
    "            for rel in triples_to_get:\n",
    "                if rel['predicate']['boundaries'] == None:\n",
    "                    rel['predicate']['boundaries'] = sentence_boundaries[i]\n",
    "                    \n",
    "            sentence_dict['triples'] = triples_to_get\n",
    "            sentence_dict['boundaries'] = sentence_boundaries[i]\n",
    "            sentence_list.append(sentence_dict)\n",
    "    return sentence_list\n",
    "    \n",
    "    \n",
    "\n",
    "#the next two functions are used to extract the embeddings from tokens / sentences\n",
    "def get_hidden_states(encoded, model, layers):\n",
    "    with torch.no_grad():\n",
    "         output = model(**encoded)\n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_words_vector(sent, tokenizer, model, layers):\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "    # get all token idxs that belong to the word of interest\n",
    "    #token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "    return get_hidden_states(encoded, model, layers)\n",
    "\n",
    "def get_idx(string_list, boundaries, token_offsets):\n",
    "    ids = []\n",
    "    for r in range(len(string_list)):\n",
    "        len_string = len(' '.join(string_list[r:]))\n",
    "        offset = boundaries[1]-len_string\n",
    "        ids.append(token_offsets[offset][0])\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ad64cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentences = []\n",
    "for file in data:\n",
    "    for doc in file:\n",
    "        for sentence in get_relations(doc):\n",
    "            data_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8903b3",
   "metadata": {},
   "source": [
    "for the sentence graph, we need:\n",
    "- <del> edge index\n",
    "- <del> edge labels / type\n",
    "- <del> node features\n",
    "- (maybe) edge feature\n",
    "\n",
    "\n",
    "- <del> dataframes [id_sentence, sentence_graph, sentence_string] || [id_sentence, relation, e1_node, e2_node] || \n",
    "- <del> max num of nodes, num of dependencies relations (inside the graph), dimensionality (300), num of relations to predict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4adcc397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 / 17557\r"
     ]
    }
   ],
   "source": [
    "dict_sentences = {}\n",
    "graphs = []\n",
    "full_rels = []\n",
    "\n",
    "df_sent_columns = ['id_sentence', 'sentence_graph', 'sentence_string']\n",
    "df_rel_columns = ['id_sentence', 'relation_uri', 'relation_boundaries', 'e1_node', 'e2_node']\n",
    "\n",
    "df_sent = []\n",
    "df_rel = []\n",
    "\n",
    "\n",
    "for enum_sent, sentence in enumerate(data_sentences[:20]):\n",
    "    print(enum_sent, len(data_sentences), sep= ' / ', end='\\r')\n",
    "    g = nx.Graph()\n",
    "    dict_embeddings = {}\n",
    "    edge_list = []\n",
    "    starting_token = 0\n",
    "    relations_list = []\n",
    "\n",
    "    sentence_string = sentence['sentence'].replace('  ', ' ')\n",
    "    re.sub('\\W+',' ',sentence_string).strip()\n",
    "    sentence_boundaries = sentence['boundaries']\n",
    "    triples = sentence['triples']\n",
    "    token_offsets = {}\n",
    "    \n",
    "    sent_embeddings = get_words_vector(sentence_string, tokenizer, model, layers)\n",
    "    doc_spacy = nlp_en(sentence_string)\n",
    "    \n",
    "    for token in doc_spacy:\n",
    "        #print('>', repr(token))\n",
    "        enum_idx = 0\n",
    "        token_offsets[token.idx] = (token.i, token.text)\n",
    "        token_idx = tokenizer.encode(token.text, add_special_tokens=False)\n",
    "        \n",
    "        token_embeddings = []\n",
    "        for enum_idx, token_id in enumerate(token_idx):\n",
    "            #print(enum_idx,\n",
    "            #      token_id,\n",
    "            #      token.idx,\n",
    "            #      token.i+skipped_tokens,\n",
    "            #      sent_embeddings.shape,\n",
    "            #      tokenizer.convert_ids_to_tokens(token_id))\n",
    "            token_embeddings.append(sent_embeddings[starting_token + enum_idx])\n",
    "            \n",
    "        starting_token += 1\n",
    "        if len(token_embeddings) > 1:\n",
    "            token_embeddings = torch.stack(token_embeddings).to(device)\n",
    "            token_embeddings = torch.mean(token_embeddings, -2)\n",
    "\n",
    "        elif len(token_embeddings) == 1:\n",
    "            token_embeddings = torch.stack(token_embeddings).to(device)\n",
    "        else:\n",
    "            token_embeddings = torch.rand(1,768)\n",
    "\n",
    "        token_embeddings = torch.reshape(token_embeddings, (1,768))\n",
    "        dict_embeddings[token.i] = token_embeddings        \n",
    "        start_token = token.idx\n",
    "        end_token = token.idx + len(token.text)\n",
    "        \n",
    "        g.add_node(token.i, label=token.text, type='token', features=token_embeddings, boundaries=(start_token, end_token))\n",
    "        edge_list.append((token.i, token.head.i, token.dep_))\n",
    "    \n",
    "    for edge in edge_list:\n",
    "        g.add_edge(edge[0], edge[1], label=edge[2])\n",
    "    row_sent = [enum_sent, g, sentence_string]\n",
    "    df_sent.append(row_sent)\n",
    "    \n",
    "    \n",
    "    next_i = token.i + 1\n",
    "    \n",
    "    for triple in triples:\n",
    "\n",
    "        try:\n",
    "            subj_uri = triple['subject']['uri']\n",
    "            subj_string = triple['subject']['surfaceform']\n",
    "            subj_string_list = subj_string.split()\n",
    "            subj_boundaries = [x - sentence_boundaries[0] for x in triple['subject']['boundaries']]\n",
    "            \n",
    "            subj_ids = get_idx(subj_string_list, subj_boundaries, token_offsets)\n",
    "\n",
    "            obj_uri = triple['object']['uri']\n",
    "            obj_string = triple['object']['surfaceform']\n",
    "            obj_string_list = obj_string.split()\n",
    "            obj_boundaries = [x - sentence_boundaries[0] for x in triple['object']['boundaries']]\n",
    "            obj_ids = get_idx(obj_string_list, obj_boundaries, token_offsets)\n",
    "\n",
    "            subj_i = int(next_i)\n",
    "            obj_i = int(next_i+1)\n",
    "\n",
    "\n",
    "            ##-----these might be useful in future------\n",
    "            rel_boundaries = triple['predicate']['boundaries']\n",
    "            rel_surfaceform = triple['predicate']['surfaceform']\n",
    "            rel_uri = triple['predicate']['uri']\n",
    "            relations_list.append((subj_i, obj_i, rel_uri))\n",
    "\n",
    "\n",
    "            text = data[0][triple['sentence_id']]['text']\n",
    "            ##----------------------------------------------\n",
    "\n",
    "            row_rel = [enum_sent,rel_uri, rel_boundaries, subj_ids, obj_ids]\n",
    "            df_rel.append(row_rel)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "df_rel = pd.DataFrame(df_rel, columns=df_rel_columns)\n",
    "df_sent = pd.DataFrame(df_sent, columns = df_sent_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "64e96b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_num_of_nodes': 75,\n",
       " 'num_of_graph_relations': 45,\n",
       " 'num_of_dimension': 300,\n",
       " 'num_of_relations_to_predict': 23}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    \"max_num_of_nodes\" : max([len(g.nodes) for g in df_sent['sentence_graph']]),\n",
    "    \"num_of_graph_relations\" : len(nlp_en.get_pipe(\"parser\").labels),\n",
    "    \"num_of_dimension\" : 300,\n",
    "    \"num_of_relations_to_predict\": len(set([x for x in df_rel['relation_uri']]))    \n",
    "}\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "99ec9ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P31', 'annotator': 'NoSubject-Triple-aligner'}, 'object': {'boundaries': [94, 109], 'surfaceform': 'language family', 'uri': 'http://www.wikidata.org/entity/Q25295', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [4, 27], 'surfaceform': 'Austroasiatic languages', 'uri': 'http://www.wikidata.org/entity/Q33199', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'NoSubject-Triple-aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P31', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [94, 109], 'surfaceform': 'language family', 'uri': 'http://www.wikidata.org/entity/Q25295', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [4, 27], 'surfaceform': 'Austroasiatic languages', 'uri': 'http://www.wikidata.org/entity/Q33199', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P31', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [94, 109], 'surfaceform': 'language family', 'uri': 'http://www.wikidata.org/entity/Q25295', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [71, 80], 'surfaceform': 'Mon–Khmer', 'uri': 'http://www.wikidata.org/entity/Q33199', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [174, 184], 'surfaceform': 'Bangladesh', 'uri': 'http://www.wikidata.org/entity/Q902', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [174, 184], 'surfaceform': 'Bangladesh', 'uri': 'http://www.wikidata.org/entity/Q902', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [174, 184], 'surfaceform': 'Bangladesh', 'uri': 'http://www.wikidata.org/entity/Q902', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [174, 184], 'surfaceform': 'Bangladesh', 'uri': 'http://www.wikidata.org/entity/Q902', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 1, 'predicate': {'boundaries': [226, 319], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P361', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [293, 297], 'surfaceform': 'Asia', 'uri': 'http://www.wikidata.org/entity/Q48', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [307, 317], 'surfaceform': 'South Asia', 'uri': 'http://www.wikidata.org/entity/Q771405', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "307",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [139]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(triple)\n\u001b[1;32m     65\u001b[0m subj_boundaries \u001b[38;5;241m=\u001b[39m triple[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboundaries\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m subj_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubj_string_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubj_boundaries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m obj_uri \u001b[38;5;241m=\u001b[39m triple[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muri\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     69\u001b[0m obj_string \u001b[38;5;241m=\u001b[39m triple[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurfaceform\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36mget_idx\u001b[0;34m(string_list, boundaries, token_offsets)\u001b[0m\n\u001b[1;32m     62\u001b[0m     len_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(string_list[r:]))\n\u001b[1;32m     63\u001b[0m     offset \u001b[38;5;241m=\u001b[39m boundaries[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mlen_string\n\u001b[0;32m---> 64\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtoken_offsets\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 307"
     ]
    }
   ],
   "source": [
    "############## This cell should be ignored\n",
    "\n",
    "dict_sentences = {}\n",
    "graphs = []\n",
    "full_rels = []\n",
    "\n",
    "df_sent_columns = ['id_sentence', 'sentence_graph', 'sentence_string']\n",
    "df_rel_columns = ['id_sentence', 'relation', 'e1_node', 'e2_node', 'boundaries']\n",
    "\n",
    "df_sent = []\n",
    "df_rel = []\n",
    "\n",
    "for enum_sent, sentence in enumerate(data_sentences):\n",
    "    g = nx.Graph()\n",
    "    dict_embeddings = {}\n",
    "    edge_list = []\n",
    "    skipped_tokens = 0\n",
    "    relations_list = []\n",
    "\n",
    "    sentence_string = sentence['sentence']\n",
    "    triples = sentence['triples']\n",
    "    token_offsets = {}\n",
    "    \n",
    "    sent_embeddings = get_words_vector(sentence_string, tokenizer, model, layers)\n",
    "    doc_spacy = nlp_en(sentence_string)\n",
    "    \n",
    "    for token in doc_spacy:\n",
    "        token_offsets[token.idx] = (token.i, token.text)\n",
    "        token_idx = tokenizer.encode(token.text, add_special_tokens=False)\n",
    "        \n",
    "        token_embeddings = []\n",
    "        for enum_idx, token_id in enumerate(token_idx):\n",
    "            token_embeddings.append(sent_embeddings[token.i+enum_idx+skipped_tokens])\n",
    "            \n",
    "        skipped_tokens += enum_idx\n",
    "\n",
    "            \n",
    "        if len(token_embeddings) > 1:\n",
    "            token_embeddings = torch.stack(token_embeddings).to(device)\n",
    "            token_embeddings = torch.mean(token_embeddings, -2)\n",
    "\n",
    "        else:\n",
    "            token_embeddings = token_embeddings[0]\n",
    "\n",
    "        token_embeddings = torch.reshape(token_embeddings, (1,768))\n",
    "\n",
    "        dict_embeddings[token.i] = token_embeddings\n",
    "        \n",
    "        start_token = token.idx\n",
    "        end_token = token.idx + len(token.text)\n",
    "        \n",
    "        g.add_node(token.i, label=token.text, type='token', features=token_embeddings, boundaries=(start_token, end_token))\n",
    "        \n",
    "        edge_list.append((token.i, token.head.i, token.dep_))\n",
    "        \n",
    "    row_sent = [enum_sent, g, sentence_string]\n",
    "    df_sent.append(row_sent)\n",
    "    \n",
    "    \n",
    "    next_i = token.i + 1\n",
    "    \n",
    "    for triple in triples:\n",
    "        subj_uri = triple['subject']['uri']\n",
    "        subj_string = triple['subject']['surfaceform']\n",
    "        subj_string_list = subj_string.split()\n",
    "        print(triple)\n",
    "        subj_boundaries = triple['subject']['boundaries']\n",
    "        subj_ids = get_idx(subj_string_list, subj_boundaries, token_offsets)\n",
    "\n",
    "        obj_uri = triple['object']['uri']\n",
    "        obj_string = triple['object']['surfaceform']\n",
    "        obj_string_list = obj_string.split()\n",
    "        obj_boundaries = triple['object']['boundaries']\n",
    "        obj_ids = get_idx(obj_string_list, obj_boundaries, token_offsets)\n",
    "\n",
    "        #embeddings_subj = []\n",
    "        #embeddings_obj = []\n",
    "\n",
    "        subj_i = int(next_i)\n",
    "        obj_i = int(next_i+1)\n",
    "        #g.add_node(subj_i, label=subj_string, type='entity')\n",
    "        \n",
    "        #for i in subj_ids:\n",
    "            #embeddings_subj.append(dict_embeddings[i])\n",
    "            #edge_list.append((i, subj_i))\n",
    "\n",
    "        #g.add_node(obj_i, label=obj_string, type='entity')\n",
    "        #for i in obj_ids:\n",
    "        #    embeddings_obj.append(dict_embeddings[i])\n",
    "        #    edge_list.append((i, obj_i))\n",
    "\n",
    "        #if len(embeddings_subj) > 1:\n",
    "        #    embeddings_subj = torch.stack(embeddings_subj).to(device)\n",
    "        #    embeddings_subj = torch.mean(embeddings_subj, -2)\n",
    "        #else:\n",
    "        #    embeddings_subj = torch.stack(embeddings_subj).to(device)\n",
    "\n",
    "\n",
    "        #if len(embeddings_obj) > 1:\n",
    "        #    embeddings_obj = torch.stack(embeddings_obj).to(device)\n",
    "        #    embeddings_obj = torch.mean(embeddings_obj, -2)\n",
    "        #else:\n",
    "        #    embeddings_obj = torch.stack(embeddings_obj).to(device) \n",
    "\n",
    "\n",
    "        #dict_embeddings[subj_i] = embeddings_subj\n",
    "        #dict_embeddings[obj_i] = embeddings_obj\n",
    "\n",
    "        #next_i += 2\n",
    "\n",
    "        ##-----these might be useful in future------\n",
    "        rel_boundaries = triple['predicate']['boundaries']\n",
    "        rel_surfaceform = triple['predicate']['surfaceform']\n",
    "        rel_uri = triple['predicate']['uri']\n",
    "        relations_list.append((subj_i, obj_i, rel_uri))\n",
    "\n",
    "\n",
    "        text = data[0][triple['sentence_id']]['text']\n",
    "        ##----------------------------------------------\n",
    "        \n",
    "        row_rel = [enum_sent,subj_ids, obj_ids, rel_boundaries]\n",
    "        df_rel.append(row_rel)\n",
    "\n",
    "        #predicate_string = data[0][0]['text'][min(obj_boundaries[0], subj_boundaries[0]):max(obj_boundaries[1],subj_boundaries[1])] if rel_boundaries == None else data[0][0]['text'][rel_boundaries[0]:rel_boundaries[1]]    \n",
    "\n",
    "    #g.add_edges_from(edge_list)\n",
    "    #graphs.append(g)\n",
    "    #full_rels.append(relations_list)\n",
    "    #dict_sentence[enum_doc] = dict_embeddings\n",
    "\n",
    "df_rel = pd.DataFrame(df_rel, columns=df_rel_columns)\n",
    "df_sent = pd.DataFrame(df_sent, columns = df_sent_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8767c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
