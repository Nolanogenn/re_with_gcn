{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "3e11c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdflib import URIRef, BNode, Literal, Namespace\n",
    "from rdflib.namespace import DCTERMS, RDFS\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "# if we use embedding only from last layer, this should stay as it is\n",
    "# it could be changed for some experiments ?\n",
    "layers = [-1]\n",
    "\n",
    "#we load the model\n",
    "#we could experiment with other models as well\n",
    "model = AutoModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "525e55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [json.load(open(x)) for x in glob.glob('./data/*.json')[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f88ec4",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "- <del> split the texts into sentences\n",
    "- <del> filter the sentences that have 2+ entities AND have a relations among these\n",
    "\n",
    "\n",
    "- baseline prediction (matching the blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "e38740cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general functions\n",
    "#the device variable can be changed in case a GPU is available\n",
    "device = torch.device('cpu')\n",
    "#uncomment the next line to use gpu\n",
    "#device = torch.device('gpu')\n",
    "\n",
    "def split_sentences(sample):\n",
    "    sentence_boundaries = sample['sentences_boundaries']\n",
    "    sentences = []\n",
    "    text = sample[\"text\"]\n",
    "    for boundary in sentence_boundaries:\n",
    "        start= boundary[0]\n",
    "        end = boundary [1]\n",
    "        sentence = text[start:end]\n",
    "        sentences.append(sentence)\n",
    "    return sentences, sentence_boundaries\n",
    "\n",
    "def get_relations(sample):\n",
    "    sentence_list = []\n",
    "    sentences, sentence_boundaries = split_sentences(sample)\n",
    "    triples = sample['triples']\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_dict = {}\n",
    "        #it looks like some entities do not have boundaries, this would make it difficult to retrieve the tokens\n",
    "        #let's just not include them for now\n",
    "        triples_to_get = [x for x in triples if x['sentence_id'] == i and x['object']['boundaries'] != None and x['subject']['boundaries'] != None]\n",
    "        if len(triples_to_get) >= 1:\n",
    "            sentence_dict['sentence'] = sentence\n",
    "            for rel in triples_to_get:\n",
    "                if rel['predicate']['boundaries'] == None:\n",
    "                    rel['predicate']['boundaries'] = sentence_boundaries[i]\n",
    "                    \n",
    "            sentence_dict['triples'] = triples_to_get\n",
    "            sentence_dict['boundaries'] = sentence_boundaries[i]\n",
    "            sentence_list.append(sentence_dict)\n",
    "    return sentence_list\n",
    "    \n",
    "\n",
    "#the next two functions are used to extract the embeddings from tokens / sentences\n",
    "def get_hidden_states(encoded, model, layers):\n",
    "    with torch.no_grad():\n",
    "         output = model(**encoded)\n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_words_vector(sent, tokenizer, model, layers):\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "    # get all token idxs that belong to the word of interest\n",
    "    #token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "    return get_hidden_states(encoded, model, layers)\n",
    "\n",
    "def get_idx(string_list, boundaries, token_offsets):\n",
    "    ids = []\n",
    "    for r in range(len(string_list)):\n",
    "        len_string = len(' '.join(string_list[r:]))\n",
    "        offset = boundaries[1]-len_string\n",
    "        ids.append(token_offsets[offset][0])\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "2ad64cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentences = []\n",
    "for file in data:\n",
    "    for doc in file:\n",
    "        for sentence in get_relations(doc):\n",
    "            data_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "e786a60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "17557"
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8903b3",
   "metadata": {},
   "source": [
    "for the sentence graph, we need:\n",
    "- <del> edge index\n",
    "- <del> edge labels / type\n",
    "- <del> node features\n",
    "- (maybe) edge feature\n",
    "\n",
    "\n",
    "- <del> dataframes [id_sentence, sentence_graph, sentence_string] || [id_sentence, relation, e1_node, e2_node] ||\n",
    "- <del> max num of nodes, num of dependencies relations (inside the graph), dimensionality (300), num of relations to predict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "4adcc397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17555 / 17557\r5578499 / 175578503 / 175578508 / 175578511 / 175578514 / 175578517 / 175578521 / 175578525 / 175578529 / 175578532 / 175578536 / 175578540 / 175578543 / 175578546 / 175578549 / 175578554 / 175578557 / 175578560 / 175578563 / 175578567 / 175578572 / 175578576 / 175578580 / 175578583 / 175578587 / 175578590 / 175578593 / 175578596 / 175578600 / 175578603 / 175578607 / 175578610 / 175578614 / 175578618 / 175578621 / 175578625 / 175578629 / 175578633 / 175578637 / 175578642 / 175578645 / 175578649 / 175578653 / 175578657 / 175578661 / 175578665 / 175578670 / 175578673 / 175578677 / 175578681 / 175578686 / 175578690 / 175578695 / 175578699 / 175578703 / 175578707 / 175578711 / 175578715 / 175578719 / 175578722 / 175578725 / 175578730 / 175578735 / 175578740 / 175578743 / 175578747 / 175578750 / 175578753 / 175578757 / 175578761 / 175578765 / 175578768 / 175578772 / 175578776 / 175578780 / 175578784 / 175578789 / 175578793 / 175578796 / 175578800 / 175578804 / 175578807 / 175578810 / 175578814 / 175578818 / 175578823 / 175578827 / 175578831 / 175578834 / 175578838 / 175578842 / 175578846 / 175578850 / 175578854 / 175578857 / 175578860 / 175578864 / 175578867 / 175578871 / 175578876 / 175578881 / 175578885 / 175578889 / 175578892 / 175578897 / 175578901 / 175578905 / 175578908 / 175578911 / 175578914 / 175578918 / 175578921 / 175578925 / 175578928 / 175578931 / 175578935 / 175578939 / 175578944 / 175578948 / 175578952 / 175578956 / 175578960 / 175578964 / 175578968 / 175578972 / 175578975 / 175578979 / 175578983 / 175578987 / 175578992 / 175578996 / 175579000 / 175579003 / 175579006 / 175579010 / 175579014 / 175579018 / 175579021 / 175579024 / 175579028 / 175579032 / 175579036 / 175579040 / 175579043 / 175579047 / 175579051 / 175579055 / 175579059 / 175579063 / 175579066 / 175579070 / 175579074 / 175579077 / 175579080 / 175579083 / 175579087 / 175579091 / 175579093 / 175579098 / 175579102 / 175579105 / 175579110 / 175579114 / 175579118 / 175579122 / 175579127 / 175579130 / 175579135 / 175579140 / 175579144 / 175579147 / 175579151 / 175579155 / 175579158 / 175579161 / 175579165 / 175579168 / 175579172 / 175579175 / 175579179 / 175579182 / 175579186 / 175579190 / 175579193 / 175579196 / 175579200 / 175579204 / 175579208 / 175579212 / 175579215 / 175579220 / 175579224 / 175579229 / 175579232 / 175579236 / 175579240 / 175579244 / 175579248 / 175579252 / 175579255 / 175579258 / 175579261 / 175579264 / 175579267 / 175579270 / 175579274 / 175579278 / 175579282 / 175579286 / 175579290 / 175579294 / 175579298 / 175579302 / 175579306 / 175579309 / 175579312 / 175579317 / 175579321 / 175579325 / 175579329 / 175579332 / 175579334 / 175579338 / 175579342 / 175579346 / 175579351 / 175579355 / 175579359 / 175579363 / 175579367 / 175579372 / 175579375 / 175579379 / 175579383 / 175579388 / 175579391 / 175579394 / 175579398 / 175579402 / 175579405 / 175579408 / 175579411 / 175579415 / 175579418 / 175579422 / 175579426 / 175579431 / 175579435 / 175579439 / 175579443 / 175579448 / 175579453 / 175579456 / 175579460 / 175579463 / 175579467 / 175579471 / 175579474 / 175579477 / 175579480 / 175579483 / 175579487 / 175579492 / 175579495 / 175579499 / 175579502 / 175579507 / 175579511 / 175579515 / 175579519 / 175579524 / 175579527 / 175579530 / 175579535 / 175579539 / 175579543 / 175579547 / 175579551 / 175579554 / 175579557 / 175579561 / 175579565 / 175579568 / 175579571 / 175579575 / 175579579 / 175579583 / 175579587 / 175579591 / 175579595 / 175579599 / 175579603 / 175579606 / 175579610 / 175579614 / 175579618 / 175579621 / 175579624 / 175579627 / 175579631 / 175579635 / 175579639 / 175579643 / 175579647 / 175579651 / 175579654 / 175579657 / 175579660 / 175579664 / 175579668 / 175579671 / 175579675 / 175579678 / 175579682 / 175579684 / 175579687 / 175579690 / 175579694 / 175579698 / 175579702 / 175579706 / 175579711 / 175579716 / 175579720 / 175579724 / 175579728 / 175579732 / 175579736 / 175579740 / 175579744 / 175579749 / 175579753 / 175579756 / 175579760 / 175579764 / 175579769 / 175579773 / 175579777 / 175579779 / 175579782 / 175579786 / 175579791 / 175579795 / 175579798 / 175579802 / 175579806 / 175579811 / 175579815 / 175579820 / 175579824 / 175579828 / 175579831 / 175579835 / 175579840 / 175579845 / 175579849 / 175579853 / 175579857 / 175579861 / 175579864 / 175579867 / 175579871 / 175579875 / 175579880 / 175579885 / 175579889 / 175579893 / 175579898 / 175579903 / 175579907 / 175579912 / 175579915 / 175579919 / 175579923 / 175579926 / 175579930 / 175579933 / 175579937 / 175579941 / 175579945 / 175579949 / 175579952 / 175579956 / 175579960 / 175579964 / 175579968 / 175579970 / 175579973 / 175579976 / 175579981 / 175579986 / 175579990 / 175579994 / 175579998 / 1755710003 / 1755710007 / 1755710011 / 1755710014 / 1755710019 / 1755710023 / 1755710027 / 1755710029 / 1755710032 / 1755710036 / 1755710040 / 1755710043 / 1755710047 / 1755710050 / 1755710053 / 1755710058 / 1755710060 / 1755710064 / 1755710068 / 1755710073 / 1755710077 / 1755710081 / 1755710085 / 1755710088 / 1755710091 / 1755710095 / 1755710099 / 1755710102 / 1755710107 / 1755710111 / 1755710115 / 1755710118 / 1755710122 / 1755710126 / 1755710130 / 1755710134 / 1755710137 / 1755710140 / 1755710143 / 1755710148 / 1755710152 / 1755710157 / 1755710162 / 1755710166 / 1755710171 / 1755710175 / 1755710178 / 1755710182 / 1755710186 / 1755710190 / 1755710195 / 1755710199 / 1755710202 / 1755710206 / 1755710209 / 1755710212 / 1755710216 / 1755710219 / 1755710223 / 1755710227 / 1755710230 / 1755710234 / 1755710238 / 1755710242 / 1755710246 / 1755710250 / 1755710253 / 1755710255 / 1755710258 / 1755710261 / 1755710266 / 1755710269 / 1755710273 / 1755710277 / 1755710280 / 1755710284 / 1755710287 / 1755710292 / 1755710296 / 1755710299 / 1755710303 / 1755710307 / 1755710310 / 1755710313 / 1755710315 / 1755710319 / 1755710324 / 1755710328 / 1755710331 / 1755710334 / 1755710338 / 1755710342 / 1755710346 / 1755710349 / 1755710352 / 1755710357 / 1755710361 / 1755710364 / 1755710368 / 1755710371 / 1755710375 / 1755710379 / 1755710383 / 1755710387 / 1755710391 / 1755710395 / 1755710400 / 1755710405 / 1755710409 / 1755710414 / 1755710417 / 1755710421 / 1755710425 / 1755710429 / 1755710433 / 1755710437 / 1755710441 / 1755710445 / 1755710449 / 1755710453 / 1755710456 / 1755710461 / 1755710465 / 1755710470 / 1755710475 / 1755710479 / 1755710484 / 1755710487 / 1755710491 / 1755710495 / 1755710498 / 1755710502 / 1755710506 / 1755710510 / 1755710515 / 1755710519 / 1755710524 / 1755710528 / 1755710532 / 1755710535 / 1755710539 / 1755710543 / 1755710546 / 1755710550 / 1755710554 / 1755710559 / 1755710563 / 1755710567 / 1755710571 / 1755710576 / 1755710580 / 1755710584 / 1755710587 / 1755710591 / 1755710595 / 1755710599 / 1755710602 / 1755710606 / 1755710610 / 1755710613 / 1755710617 / 1755710621 / 1755710625 / 1755710630 / 1755710634 / 1755710638 / 1755710643 / 1755710646 / 1755710650 / 1755710654 / 1755710659 / 1755710664 / 1755710668 / 1755710672 / 1755710676 / 1755710681 / 1755710685 / 1755710689 / 1755710693 / 1755710697 / 1755710701 / 1755710705 / 1755710709 / 1755710713 / 1755710717 / 1755710721 / 1755710725 / 1755710729 / 1755710733 / 1755710737 / 1755710742 / 1755710746 / 1755710750 / 1755710753 / 1755710757 / 1755710761 / 1755710765 / 1755710770 / 1755710773 / 1755710776 / 1755710779 / 1755710782 / 1755710786 / 1755710790 / 1755710793 / 1755710797 / 1755710802 / 1755710806 / 1755710810 / 1755710814 / 1755710819 / 1755710822 / 1755710825 / 1755710828 / 1755710832 / 1755710836 / 1755710839 / 1755710843 / 1755710847 / 1755710850 / 1755710854 / 1755710857 / 1755710860 / 1755710864 / 1755710868 / 1755710872 / 1755710876 / 1755710880 / 1755710883 / 1755710886 / 1755710890 / 1755710894 / 1755710897 / 1755710900 / 1755710904 / 1755710908 / 1755710912 / 1755710916 / 1755710920 / 1755710923 / 1755710927 / 1755710931 / 1755710935 / 1755710939 / 1755710943 / 1755710947 / 1755710951 / 1755710956 / 1755710960 / 1755710965 / 1755710968 / 1755710972 / 1755710977 / 1755710982 / 1755710987 / 1755710991 / 1755710996 / 1755711000 / 1755711004 / 1755711008 / 1755711011 / 1755711015 / 1755711019 / 1755711023 / 1755711028 / 1755711033 / 1755711038 / 1755711043 / 1755711048 / 1755711052 / 1755711057 / 1755711062 / 1755711065 / 1755711069 / 1755711073 / 1755711077 / 1755711080 / 1755711084 / 1755711088 / 1755711092 / 1755711097 / 1755711102 / 1755711106 / 1755711109 / 1755711114 / 1755711117 / 1755711121 / 1755711125 / 1755711129 / 1755711132 / 1755711136 / 1755711141 / 1755711146 / 1755711151 / 1755711155 / 1755711159 / 1755711162 / 1755711165 / 1755711168 / 1755711172 / 1755711176 / 1755711181 / 1755711185 / 1755711190 / 1755711195 / 1755711200 / 1755711204 / 1755711209 / 1755711213 / 1755711218 / 1755711222 / 1755711226 / 1755711228 / 1755711233 / 1755711237 / 1755711242 / 1755711246 / 1755711250 / 1755711254 / 1755711258 / 1755711263 / 1755711267 / 1755711271 / 1755711276 / 1755711281 / 1755711284 / 1755711287 / 1755711292 / 1755711296 / 1755711301 / 1755711305 / 1755711310 / 1755711314 / 1755711318 / 1755711321 / 1755711325 / 1755711328 / 1755711332 / 1755711336 / 1755711340 / 1755711344 / 1755711349 / 1755711353 / 1755711357 / 1755711359 / 1755711362 / 1755711365 / 1755711370 / 1755711375 / 1755711379 / 1755711383 / 1755711386 / 1755711390 / 1755711394 / 1755711398 / 1755711403 / 1755711407 / 1755711410 / 1755711414 / 1755711418 / 1755711422 / 1755711426 / 1755711431 / 1755711435 / 1755711439 / 1755711443 / 1755711445 / 1755711449 / 1755711454 / 1755711458 / 1755711461 / 1755711465 / 1755711468 / 1755711472 / 1755711476 / 1755711479 / 1755711483 / 1755711487 / 1755711491 / 1755711494 / 1755711498 / 1755711503 / 1755711507 / 1755711510 / 1755711514 / 1755711519 / 1755711523 / 1755711527 / 1755711531 / 1755711536 / 1755711540 / 1755711545 / 1755711549 / 1755711553 / 1755711557 / 1755711560 / 1755711563 / 1755711566 / 1755711569 / 1755711572 / 1755711575 / 1755711579 / 1755711583 / 1755711587 / 1755711591 / 1755711594 / 1755711598 / 1755711600 / 1755711603 / 1755711607 / 1755711611 / 1755711616 / 1755711621 / 1755711625 / 1755711629 / 1755711633 / 1755711637 / 1755711641 / 1755711645 / 1755711650 / 1755711654 / 1755711658 / 1755711661 / 1755711665 / 1755711669 / 1755711673 / 1755711677 / 1755711682 / 1755711687 / 1755711691 / 1755711694 / 1755711697 / 1755711700 / 1755711704 / 1755711708 / 1755711712 / 1755711716 / 1755711721 / 1755711725 / 1755711729 / 1755711733 / 1755711737 / 1755711741 / 1755711745 / 1755711748 / 1755711752 / 1755711756 / 1755711760 / 1755711763 / 1755711767 / 1755711770 / 1755711774 / 1755711779 / 1755711783 / 1755711788 / 1755711791 / 1755711796 / 1755711801 / 1755711805 / 1755711808 / 1755711812 / 1755711816 / 1755711820 / 1755711824 / 1755711829 / 1755711834 / 1755711838 / 1755711842 / 1755711846 / 1755711850 / 1755711855 / 1755711859 / 1755711864 / 1755711869 / 1755711873 / 1755711878 / 1755711882 / 1755711887 / 1755711891 / 1755711895 / 1755711899 / 1755711903 / 1755711906 / 1755711909 / 1755711913 / 1755711916 / 1755711920 / 1755711924 / 1755711928 / 1755711931 / 1755711934 / 1755711938 / 1755711941 / 1755711946 / 1755711950 / 1755711955 / 1755711960 / 1755711964 / 1755711967 / 1755711971 / 1755711975 / 1755711980 / 1755711984 / 1755711987 / 1755711991 / 1755711995 / 1755712000 / 1755712004 / 1755712008 / 1755712012 / 1755712015 / 1755712018 / 1755712022 / 1755712026 / 1755712029 / 1755712033 / 1755712037 / 1755712041 / 1755712045 / 1755712049 / 1755712053 / 1755712057 / 1755712061 / 1755712065 / 1755712068 / 1755712072 / 1755712076 / 1755712081 / 1755712085 / 1755712090 / 1755712094 / 1755712098 / 1755712102 / 1755712106 / 1755712110 / 1755712113 / 1755712116 / 1755712120 / 1755712124 / 1755712127 / 1755712131 / 1755712134 / 1755712137 / 1755712142 / 1755712145 / 1755712149 / 1755712153 / 1755712157 / 1755712162 / 1755712166 / 1755712170 / 1755712173 / 1755712177 / 1755712182 / 1755712186 / 1755712190 / 1755712195 / 1755712200 / 1755712204 / 1755712208 / 1755712213 / 1755712218 / 1755712222 / 1755712226 / 1755712229 / 1755712233 / 1755712237 / 1755712241 / 1755712245 / 1755712248 / 1755712252 / 1755712256 / 1755712260 / 1755712264 / 1755712268 / 1755712272 / 1755712277 / 1755712281 / 1755712286 / 1755712291 / 1755712295 / 1755712298 / 1755712302 / 1755712306 / 1755712310 / 1755712313 / 1755712316 / 1755712320 / 1755712323 / 1755712327 / 1755712332 / 1755712336 / 1755712341 / 1755712345 / 1755712348 / 1755712352 / 1755712357 / 1755712361 / 1755712365 / 1755712370 / 1755712374 / 1755712379 / 1755712383 / 1755712387 / 1755712391 / 1755712395 / 1755712399 / 1755712403 / 1755712407 / 1755712411 / 1755712414 / 1755712418 / 1755712421 / 1755712426 / 1755712429 / 1755712432 / 1755712437 / 1755712440 / 1755712444 / 1755712448 / 1755712452 / 1755712456 / 1755712459 / 1755712463 / 1755712468 / 1755712472 / 1755712476 / 1755712479 / 1755712483 / 1755712487 / 1755712491 / 1755712496 / 1755712500 / 1755712505 / 1755712509 / 1755712513 / 1755712517 / 1755712520 / 1755712524 / 1755712528 / 1755712532 / 1755712536 / 1755712540 / 1755712545 / 1755712548 / 1755712552 / 1755712556 / 1755712561 / 1755712565 / 1755712569 / 1755712572 / 1755712576 / 1755712580 / 1755712584 / 1755712588 / 1755712593 / 1755712597 / 1755712601 / 1755712605 / 1755712608 / 1755712612 / 1755712617 / 1755712621 / 1755712626 / 1755712630 / 1755712634 / 1755712638 / 1755712642 / 1755712646 / 1755712649 / 1755712653 / 1755712657 / 1755712662 / 1755712666 / 1755712670 / 1755712674 / 1755712678 / 1755712681 / 1755712685 / 1755712689 / 1755712692 / 1755712697 / 1755712701 / 1755712705 / 1755712708 / 1755712711 / 1755712715 / 1755712718 / 1755712722 / 1755712726 / 1755712730 / 1755712734 / 1755712739 / 1755712743 / 1755712747 / 1755712751 / 1755712755 / 1755712759 / 1755712763 / 1755712767 / 1755712771 / 1755712775 / 1755712778 / 1755712782 / 1755712786 / 1755712790 / 1755712793 / 1755712797 / 1755712801 / 1755712805 / 1755712808 / 1755712811 / 1755712815 / 1755712820 / 1755712824 / 1755712828 / 1755712832 / 1755712834 / 1755712838 / 1755712841 / 1755712845 / 1755712850 / 1755712854 / 1755712857 / 1755712860 / 1755712864 / 1755712868 / 1755712872 / 1755712877 / 1755712882 / 1755712886 / 1755712891 / 1755712895 / 1755712899 / 1755712904 / 1755712909 / 1755712913 / 1755712916 / 1755712920 / 1755712923 / 1755712927 / 1755712931 / 1755712935 / 1755712938 / 1755712942 / 1755712946 / 1755712949 / 1755712953 / 1755712957 / 1755712961 / 1755712965 / 1755712969 / 1755712973 / 1755712976 / 1755712980 / 1755712985 / 1755712989 / 1755712993 / 1755712997 / 1755713001 / 1755713005 / 1755713009 / 1755713013 / 1755713017 / 1755713021 / 1755713024 / 1755713029 / 1755713033 / 1755713037 / 1755713040 / 1755713044 / 1755713048 / 1755713051 / 1755713055 / 1755713060 / 1755713064 / 1755713066 / 1755713070 / 1755713074 / 1755713078 / 1755713082 / 1755713087 / 1755713090 / 1755713094 / 1755713098 / 1755713103 / 1755713108 / 1755713111 / 1755713114 / 1755713119 / 1755713123 / 1755713128 / 1755713133 / 1755713137 / 1755713141 / 1755713144 / 1755713148 / 1755713152 / 1755713156 / 1755713159 / 1755713163 / 1755713167 / 1755713171 / 1755713176 / 1755713180 / 1755713184 / 1755713186 / 1755713190 / 1755713193 / 1755713196 / 1755713200 / 1755713205 / 1755713209 / 1755713214 / 1755713219 / 1755713223 / 1755713227 / 1755713231 / 1755713235 / 1755713239 / 1755713243 / 1755713247 / 1755713250 / 1755713254 / 1755713258 / 1755713262 / 1755713267 / 1755713271 / 1755713275 / 1755713279 / 1755713283 / 1755713287 / 1755713291 / 1755713295 / 1755713299 / 1755713302 / 1755713305 / 1755713308 / 1755713313 / 1755713317 / 1755713321 / 1755713325 / 1755713328 / 1755713330 / 1755713334 / 1755713338 / 1755713343 / 1755713347 / 1755713351 / 1755713355 / 1755713359 / 1755713363 / 1755713368 / 1755713372 / 1755713377 / 1755713382 / 1755713387 / 1755713391 / 1755713395 / 1755713399 / 1755713403 / 1755713407 / 1755713410 / 1755713413 / 1755713417 / 1755713421 / 1755713425 / 1755713430 / 1755713434 / 1755713438 / 1755713443 / 1755713448 / 1755713452 / 1755713456 / 1755713460 / 1755713464 / 1755713469 / 1755713473 / 1755713477 / 1755713481 / 1755713485 / 1755713488 / 1755713491 / 1755713494 / 1755713498 / 1755713502 / 1755713506 / 1755713511 / 1755713514 / 1755713517 / 1755713522 / 1755713526 / 1755713529 / 1755713533 / 1755713537 / 1755713541 / 1755713545 / 1755713548 / 1755713552 / 1755713556 / 1755713561 / 1755713566 / 1755713570 / 1755713574 / 1755713577 / 1755713582 / 1755713585 / 1755713589 / 1755713591 / 1755713594 / 1755713598 / 1755713602 / 1755713606 / 1755713610 / 1755713614 / 1755713618 / 1755713621 / 1755713625 / 1755713628 / 1755713631 / 1755713636 / 1755713639 / 1755713641 / 1755713645 / 1755713650 / 1755713654 / 1755713658 / 1755713661 / 1755713665 / 1755713669 / 1755713672 / 1755713676 / 1755713680 / 1755713684 / 1755713688 / 1755713691 / 1755713694 / 1755713698 / 1755713702 / 1755713705 / 1755713708 / 1755713711 / 1755713715 / 1755713718 / 1755713722 / 1755713726 / 1755713730 / 1755713733 / 1755713737 / 1755713741 / 1755713744 / 1755713747 / 1755713752 / 1755713756 / 1755713760 / 1755713764 / 1755713767 / 1755713770 / 1755713773 / 1755713775 / 1755713779 / 1755713782 / 1755713786 / 1755713789 / 1755713793 / 1755713798 / 1755713802 / 1755713805 / 1755713809 / 1755713812 / 1755713816 / 1755713820 / 1755713823 / 1755713827 / 1755713830 / 1755713834 / 1755713838 / 1755713842 / 1755713846 / 1755713850 / 1755713855 / 1755713859 / 1755713864 / 1755713868 / 1755713872 / 1755713875 / 1755713879 / 1755713883 / 1755713885 / 1755713889 / 1755713893 / 1755713897 / 1755713902 / 1755713906 / 1755713910 / 1755713914 / 1755713918 / 1755713921 / 1755713925 / 1755713929 / 1755713933 / 1755713937 / 1755713941 / 1755713945 / 1755713949 / 1755713952 / 1755713956 / 1755713960 / 1755713964 / 1755713968 / 1755713972 / 1755713976 / 1755713979 / 1755713982 / 1755713986 / 1755713989 / 1755713992 / 1755713997 / 1755714001 / 1755714005 / 1755714009 / 1755714013 / 1755714017 / 1755714021 / 1755714025 / 1755714029 / 1755714033 / 1755714037 / 1755714041 / 1755714045 / 1755714049 / 1755714054 / 1755714058 / 1755714063 / 1755714067 / 1755714072 / 1755714075 / 1755714079 / 1755714082 / 1755714086 / 1755714091 / 1755714095 / 1755714099 / 1755714103 / 1755714107 / 1755714111 / 1755714116 / 1755714120 / 1755714123 / 1755714126 / 1755714129 / 1755714132 / 1755714136 / 1755714140 / 1755714144 / 1755714148 / 1755714151 / 1755714155 / 1755714160 / 1755714164 / 1755714168 / 1755714171 / 1755714175 / 1755714179 / 1755714183 / 1755714187 / 1755714191 / 1755714196 / 1755714200 / 1755714204 / 1755714208 / 1755714212 / 1755714216 / 1755714220 / 1755714223 / 1755714227 / 1755714231 / 1755714234 / 1755714240 / 1755714245 / 1755714249 / 1755714253 / 1755714257 / 1755714260 / 1755714264 / 1755714268 / 1755714272 / 1755714275 / 1755714279 / 1755714283 / 1755714286 / 1755714289 / 1755714293 / 1755714297 / 1755714300 / 1755714304 / 1755714308 / 1755714313 / 1755714317 / 1755714321 / 1755714326 / 1755714330 / 1755714334 / 1755714338 / 1755714342 / 1755714345 / 1755714349 / 1755714354 / 1755714358 / 1755714363 / 1755714367 / 1755714372 / 1755714376 / 1755714381 / 1755714385 / 1755714390 / 1755714393 / 1755714397 / 1755714401 / 1755714405 / 1755714408 / 1755714413 / 1755714418 / 1755714422 / 1755714426 / 1755714430 / 1755714434 / 1755714438 / 1755714441 / 1755714446 / 1755714449 / 1755714452 / 1755714457 / 1755714460 / 1755714464 / 1755714467 / 1755714472 / 1755714475 / 1755714479 / 1755714482 / 1755714486 / 1755714490 / 1755714493 / 1755714498 / 1755714502 / 1755714506 / 1755714510 / 1755714514 / 1755714518 / 1755714523 / 1755714527 / 1755714530 / 1755714533 / 1755714537 / 1755714540 / 1755714544 / 1755714548 / 1755714552 / 1755714555 / 1755714560 / 1755714564 / 1755714567 / 1755714572 / 1755714576 / 1755714580 / 1755714584 / 1755714589 / 1755714593 / 1755714597 / 1755714600 / 1755714603 / 1755714605 / 1755714608 / 1755714611 / 1755714614 / 1755714617 / 1755714619 / 1755714623 / 1755714627 / 1755714631 / 1755714634 / 1755714638 / 1755714641 / 1755714645 / 1755714650 / 1755714653 / 1755714656 / 1755714659 / 1755714663 / 1755714668 / 1755714670 / 1755714674 / 1755714678 / 1755714681 / 1755714685 / 1755714688 / 1755714691 / 1755714694 / 1755714699 / 1755714703 / 1755714707 / 1755714711 / 1755714715 / 1755714719 / 1755714723 / 1755714727 / 1755714731 / 1755714734 / 1755714738 / 1755714742 / 1755714747 / 1755714751 / 1755714755 / 1755714759 / 1755714763 / 1755714767 / 1755714771 / 1755714774 / 1755714777 / 1755714780 / 1755714784 / 1755714788 / 1755714792 / 1755714796 / 1755714799 / 1755714804 / 1755714808 / 1755714813 / 1755714817 / 1755714822 / 1755714826 / 1755714830 / 1755714833 / 1755714836 / 1755714841 / 1755714845 / 1755714848 / 1755714852 / 1755714856 / 1755714859 / 1755714864 / 1755714868 / 1755714872 / 1755714875 / 1755714880 / 1755714884 / 1755714888 / 1755714892 / 1755714896 / 1755714900 / 1755714904 / 1755714908 / 1755714912 / 1755714916 / 1755714921 / 1755714924 / 1755714928 / 1755714933 / 1755714937 / 1755714940 / 1755714944 / 1755714948 / 1755714952 / 1755714956 / 1755714961 / 1755714965 / 1755714970 / 1755714974 / 1755714978 / 1755714981 / 1755714985 / 1755714990 / 1755714993 / 1755714996 / 1755715000 / 1755715003 / 1755715007 / 1755715010 / 1755715014 / 1755715017 / 1755715021 / 1755715024 / 1755715027 / 1755715031 / 1755715036 / 1755715039 / 1755715042 / 1755715046 / 1755715051 / 1755715056 / 1755715059 / 1755715062 / 1755715066 / 1755715069 / 1755715073 / 1755715077 / 1755715082 / 1755715086 / 1755715089 / 1755715093 / 1755715097 / 1755715101 / 1755715104 / 1755715108 / 1755715112 / 1755715115 / 1755715117 / 1755715121 / 1755715125 / 1755715129 / 1755715133 / 1755715137 / 1755715141 / 1755715145 / 1755715149 / 1755715153 / 1755715157 / 1755715163 / 1755715166 / 1755715170 / 1755715174 / 1755715178 / 1755715181 / 1755715185 / 1755715189 / 1755715192 / 1755715196 / 1755715200 / 1755715204 / 1755715208 / 1755715213 / 1755715216 / 1755715219 / 1755715223 / 1755715227 / 1755715230 / 1755715233 / 1755715237 / 1755715241 / 1755715245 / 1755715249 / 1755715253 / 1755715257 / 1755715261 / 1755715264 / 1755715267 / 1755715271 / 1755715276 / 1755715281 / 1755715284 / 1755715287 / 1755715289 / 1755715291 / 1755715295 / 1755715299 / 1755715303 / 1755715306 / 1755715309 / 1755715312 / 1755715316 / 1755715320 / 1755715323 / 1755715326 / 1755715329 / 1755715332 / 1755715335 / 1755715339 / 1755715344 / 1755715349 / 1755715352 / 1755715357 / 1755715361 / 1755715366 / 1755715370 / 1755715375 / 1755715379 / 1755715383 / 1755715386 / 1755715390 / 1755715394 / 1755715398 / 1755715402 / 1755715406 / 1755715409 / 1755715413 / 1755715418 / 1755715421 / 1755715425 / 1755715430 / 1755715433 / 1755715435 / 1755715437 / 1755715440 / 1755715443 / 1755715447 / 1755715451 / 1755715455 / 1755715458 / 1755715461 / 1755715465 / 1755715468 / 1755715472 / 1755715475 / 1755715479 / 1755715482 / 1755715485 / 1755715488 / 1755715491 / 1755715494 / 1755715496 / 1755715500 / 1755715504 / 1755715508 / 1755715512 / 1755715516 / 1755715520 / 1755715523 / 1755715526 / 1755715530 / 1755715533 / 1755715538 / 1755715542 / 1755715545 / 1755715549 / 1755715552 / 1755715556 / 1755715559 / 1755715563 / 1755715567 / 1755715570 / 1755715573 / 1755715577 / 1755715581 / 1755715585 / 1755715589 / 1755715593 / 1755715598 / 1755715602 / 1755715606 / 1755715611 / 1755715615 / 1755715619 / 1755715623 / 1755715627 / 1755715631 / 1755715635 / 1755715640 / 1755715644 / 1755715648 / 1755715652 / 1755715656 / 1755715661 / 1755715664 / 1755715668 / 1755715672 / 1755715677 / 1755715681 / 1755715685 / 1755715689 / 1755715692 / 1755715696 / 1755715700 / 1755715704 / 1755715708 / 1755715712 / 1755715715 / 1755715719 / 1755715723 / 1755715726 / 1755715731 / 1755715735 / 1755715739 / 1755715742 / 1755715746 / 1755715750 / 1755715754 / 1755715758 / 1755715761 / 1755715765 / 1755715768 / 1755715772 / 1755715776 / 1755715780 / 1755715784 / 1755715787 / 1755715791 / 1755715794 / 1755715798 / 1755715801 / 1755715805 / 1755715808 / 1755715813 / 1755715817 / 1755715821 / 1755715825 / 1755715829 / 1755715833 / 1755715837 / 1755715841 / 1755715844 / 1755715849 / 1755715852 / 1755715856 / 1755715859 / 1755715863 / 1755715867 / 1755715870 / 1755715873 / 1755715877 / 1755715880 / 1755715884 / 1755715887 / 1755715892 / 1755715896 / 1755715900 / 1755715903 / 1755715907 / 1755715910 / 1755715914 / 1755715919 / 1755715923 / 1755715928 / 1755715933 / 1755715938 / 1755715942 / 1755715946 / 1755715950 / 1755715953 / 1755715957 / 1755715960 / 1755715964 / 1755715968 / 1755715972 / 1755715976 / 1755715981 / 1755715983 / 1755715987 / 1755715992 / 1755715996 / 1755716000 / 1755716004 / 1755716008 / 1755716013 / 1755716017 / 1755716021 / 1755716025 / 1755716029 / 1755716033 / 1755716037 / 1755716041 / 1755716045 / 1755716049 / 1755716053 / 1755716057 / 1755716061 / 1755716065 / 1755716069 / 1755716074 / 1755716077 / 1755716081 / 1755716085 / 1755716089 / 1755716093 / 1755716098 / 1755716102 / 1755716106 / 1755716109 / 1755716113 / 1755716117 / 1755716121 / 1755716125 / 1755716129 / 1755716133 / 1755716137 / 1755716141 / 1755716145 / 1755716148 / 1755716152 / 1755716157 / 1755716161 / 1755716166 / 1755716171 / 1755716176 / 1755716180 / 1755716184 / 1755716189 / 1755716193 / 1755716197 / 1755716201 / 1755716205 / 1755716210 / 1755716214 / 1755716218 / 1755716222 / 1755716226 / 1755716229 / 1755716233 / 1755716237 / 1755716240 / 1755716245 / 1755716249 / 1755716253 / 1755716257 / 1755716261 / 1755716265 / 1755716269 / 1755716273 / 1755716276 / 1755716279 / 1755716282 / 1755716286 / 1755716290 / 1755716294 / 1755716298 / 1755716301 / 1755716305 / 1755716308 / 1755716312 / 1755716316 / 1755716320 / 1755716322 / 1755716325 / 1755716329 / 1755716332 / 1755716335 / 1755716338 / 1755716342 / 1755716346 / 1755716350 / 1755716353 / 1755716357 / 1755716361 / 1755716364 / 1755716368 / 1755716371 / 1755716374 / 1755716378 / 1755716382 / 1755716386 / 1755716390 / 1755716394 / 1755716397 / 1755716401 / 1755716406 / 1755716410 / 1755716414 / 1755716418 / 1755716422 / 1755716424 / 1755716428 / 1755716432 / 1755716436 / 1755716439 / 1755716442 / 1755716447 / 1755716452 / 1755716456 / 1755716461 / 1755716465 / 1755716469 / 1755716472 / 1755716475 / 1755716479 / 1755716483 / 1755716487 / 1755716490 / 1755716495 / 1755716499 / 1755716501 / 1755716505 / 1755716509 / 1755716513 / 1755716518 / 1755716522 / 1755716525 / 1755716529 / 1755716532 / 1755716536 / 1755716540 / 1755716544 / 1755716548 / 1755716552 / 1755716556 / 1755716560 / 1755716564 / 1755716568 / 1755716571 / 1755716576 / 1755716580 / 1755716583 / 1755716587 / 1755716591 / 1755716595 / 1755716599 / 1755716603 / 1755716607 / 1755716611 / 1755716615 / 1755716619 / 1755716622 / 1755716627 / 1755716630 / 1755716633 / 1755716637 / 1755716642 / 1755716646 / 1755716650 / 1755716654 / 1755716658 / 1755716662 / 1755716666 / 1755716669 / 1755716673 / 1755716677 / 1755716681 / 1755716684 / 1755716688 / 1755716692 / 1755716696 / 1755716700 / 1755716703 / 1755716705 / 1755716709 / 1755716712 / 1755716716 / 1755716720 / 1755716723 / 1755716727 / 1755716731 / 1755716735 / 1755716739 / 1755716743 / 1755716747 / 1755716751 / 1755716755 / 1755716759 / 1755716764 / 1755716768 / 1755716772 / 1755716776 / 1755716779 / 1755716783 / 1755716786 / 1755716790 / 1755716794 / 1755716798 / 1755716802 / 1755716806 / 1755716810 / 1755716814 / 1755716818 / 1755716822 / 1755716825 / 1755716829 / 1755716832 / 1755716835 / 1755716838 / 1755716842 / 1755716847 / 1755716851 / 1755716855 / 1755716860 / 1755716864 / 1755716868 / 1755716871 / 1755716876 / 1755716879 / 1755716882 / 1755716886 / 1755716889 / 1755716893 / 1755716897 / 1755716900 / 1755716904 / 1755716908 / 1755716912 / 1755716916 / 1755716920 / 1755716923 / 1755716928 / 1755716933 / 1755716937 / 1755716941 / 1755716943 / 1755716947 / 1755716951 / 1755716955 / 1755716958 / 1755716962 / 1755716966 / 1755716970 / 1755716974 / 1755716978 / 1755716983 / 1755716986 / 1755716990 / 1755716995 / 1755716998 / 1755717002 / 1755717007 / 1755717010 / 1755717015 / 1755717019 / 1755717022 / 1755717026 / 1755717030 / 1755717034 / 1755717038 / 1755717042 / 1755717046 / 1755717049 / 1755717054 / 1755717058 / 1755717060 / 1755717063 / 1755717066 / 1755717070 / 1755717074 / 1755717079 / 1755717083 / 1755717087 / 1755717090 / 1755717094 / 1755717098 / 1755717102 / 1755717105 / 1755717109 / 1755717112 / 1755717116 / 1755717121 / 1755717125 / 1755717129 / 1755717133 / 1755717136 / 1755717141 / 1755717145 / 1755717148 / 1755717152 / 1755717154 / 1755717158 / 1755717161 / 1755717164 / 1755717168 / 1755717173 / 1755717176 / 1755717179 / 1755717183 / 1755717187 / 1755717191 / 1755717195 / 1755717200 / 1755717204 / 1755717208 / 1755717212 / 1755717216 / 1755717219 / 1755717222 / 1755717225 / 1755717228 / 1755717231 / 1755717235 / 1755717239 / 1755717243 / 1755717246 / 1755717249 / 1755717253 / 1755717257 / 1755717261 / 1755717264 / 1755717267 / 1755717271 / 1755717275 / 1755717279 / 1755717282 / 1755717286 / 1755717290 / 1755717294 / 1755717299 / 1755717303 / 1755717307 / 1755717311 / 1755717315 / 1755717319 / 1755717323 / 1755717328 / 1755717332 / 1755717336 / 1755717340 / 1755717343 / 1755717347 / 1755717351 / 1755717355 / 1755717359 / 1755717362 / 1755717366 / 1755717370 / 1755717374 / 1755717378 / 1755717382 / 1755717385 / 1755717389 / 1755717394 / 1755717398 / 1755717401 / 1755717405 / 1755717409 / 1755717411 / 1755717415 / 1755717418 / 1755717422 / 1755717425 / 1755717429 / 1755717433 / 1755717436 / 1755717440 / 1755717444 / 1755717448 / 1755717452 / 1755717456 / 1755717459 / 1755717462 / 1755717466 / 1755717471 / 1755717475 / 1755717479 / 1755717482 / 1755717486 / 1755717490 / 1755717493 / 1755717497 / 1755717500 / 1755717504 / 1755717508 / 1755717513 / 1755717517 / 1755717520 / 1755717523 / 1755717527 / 1755717530 / 1755717534 / 1755717538 / 1755717542 / 1755717545 / 1755717549 / 1755717553 / 1755717556 / 17557\r"
     ]
    }
   ],
   "source": [
    "dict_sentences = {}\n",
    "graphs = []\n",
    "full_rels = []\n",
    "\n",
    "df_sent_columns = ['id_sentence', 'sentence_graph', 'sentence_string']\n",
    "df_rel_columns = ['id_sentence', 'relation_uri', 'relation_boundaries', 'e1_node', 'e2_node']\n",
    "\n",
    "df_sent = []\n",
    "df_rel = []\n",
    "\n",
    "\n",
    "for enum_sent, sentence in enumerate(data_sentences):\n",
    "    print(enum_sent, len(data_sentences), sep= ' / ', end='\\r')\n",
    "    g = nx.Graph()\n",
    "    dict_embeddings = {}\n",
    "    edge_list = []\n",
    "    starting_token = 0\n",
    "    relations_list = []\n",
    "\n",
    "    sentence_string = sentence['sentence'].replace('  ', ' ')\n",
    "    re.sub('\\W+',' ',sentence_string).strip()\n",
    "    sentence_boundaries = sentence['boundaries']\n",
    "    triples = sentence['triples']\n",
    "    token_offsets = {}\n",
    "    \n",
    "    sent_embeddings = get_words_vector(sentence_string, tokenizer, model, layers)\n",
    "    doc_spacy = nlp_en(sentence_string)\n",
    "    \n",
    "    for token in doc_spacy:\n",
    "        #print('>', repr(token))\n",
    "        enum_idx = 0\n",
    "        token_offsets[token.idx] = (token.i, token.text)\n",
    "        token_idx = tokenizer.encode(token.text, add_special_tokens=False)\n",
    "        \n",
    "        token_embeddings = []\n",
    "        for enum_idx, token_id in enumerate(token_idx):\n",
    "            #print(enum_idx,\n",
    "            #      token_id,\n",
    "            #      token.idx,\n",
    "            #      token.i+skipped_tokens,\n",
    "            #      sent_embeddings.shape,\n",
    "            #      tokenizer.convert_ids_to_tokens(token_id))\n",
    "            token_embeddings.append(sent_embeddings[starting_token + enum_idx])\n",
    "            \n",
    "        starting_token += 1\n",
    "        if len(token_embeddings) > 1:\n",
    "            token_embeddings = torch.stack(token_embeddings).to(device)\n",
    "            token_embeddings = torch.mean(token_embeddings, -2)\n",
    "\n",
    "        elif len(token_embeddings) == 1:\n",
    "            token_embeddings = torch.stack(token_embeddings).to(device)\n",
    "        else:\n",
    "            token_embeddings = torch.rand(1,768)\n",
    "\n",
    "        token_embeddings = torch.reshape(token_embeddings, (1,768))\n",
    "        dict_embeddings[token.i] = token_embeddings        \n",
    "        start_token = token.idx\n",
    "        end_token = token.idx + len(token.text)\n",
    "        \n",
    "        g.add_node(token.i, label=token.text, type='token', features=token_embeddings, boundaries=(start_token, end_token))\n",
    "        edge_list.append((token.i, token.head.i, token.dep_))\n",
    "    \n",
    "    for edge in edge_list:\n",
    "        g.add_edge(edge[0], edge[1], label=edge[2])\n",
    "    row_sent = [enum_sent, g, sentence_string]\n",
    "    df_sent.append(row_sent)\n",
    "    \n",
    "    \n",
    "    next_i = token.i + 1\n",
    "    \n",
    "    for triple in triples:\n",
    "\n",
    "        try:\n",
    "            subj_uri = triple['subject']['uri']\n",
    "            subj_string = triple['subject']['surfaceform']\n",
    "            subj_string_list = subj_string.split()\n",
    "            subj_boundaries = [x - sentence_boundaries[0] for x in triple['subject']['boundaries']]\n",
    "            \n",
    "            subj_ids = get_idx(subj_string_list, subj_boundaries, token_offsets)\n",
    "\n",
    "            obj_uri = triple['object']['uri']\n",
    "            obj_string = triple['object']['surfaceform']\n",
    "            obj_string_list = obj_string.split()\n",
    "            obj_boundaries = [x - sentence_boundaries[0] for x in triple['object']['boundaries']]\n",
    "            obj_ids = get_idx(obj_string_list, obj_boundaries, token_offsets)\n",
    "\n",
    "            relation_boundary_start = min([subj_boundaries[0], obj_boundaries[0]])\n",
    "            relation_boundary_end = max([subj_boundaries[1], obj_boundaries[1]])\n",
    "\n",
    "\n",
    "            ##-----these might be useful in future------\n",
    "            #rel_boundaries = triple['predicate']['boundaries']\n",
    "            rel_boundaries = [relation_boundary_start, relation_boundary_end]\n",
    "            rel_surfaceform = triple['predicate']['surfaceform']\n",
    "            rel_uri = triple['predicate']['uri']\n",
    "            relations_list.append((subj_ids, obj_ids, rel_uri))\n",
    "\n",
    "\n",
    "            text = data[0][triple['sentence_id']]['text']\n",
    "            ##----------------------------------------------\n",
    "            \n",
    "            subj_degrees = [g.degree(x) for x in subj_ids]\n",
    "            subj_id_index = np.argmax(subj_degrees)\n",
    "                        \n",
    "                \n",
    "            obj_degrees = [g.degree(x) for x in obj_ids]\n",
    "            obj_id_index = np.argmax(obj_degrees)\n",
    "            row_rel = [enum_sent,rel_uri, rel_boundaries, subj_ids[subj_id_index], obj_ids[obj_id_index]]\n",
    "            df_rel.append(row_rel)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "df_rel = pd.DataFrame(df_rel, columns=df_rel_columns)\n",
    "df_sent = pd.DataFrame(df_sent, columns = df_sent_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['id_sentence', 'sentence_graph', 'sentence_string'], dtype='object')"
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "fa76ed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(df_sent['sentence_graph'][0].nodes[0]['features'].size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4c002bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    id_sentence                             relation_uri relation_boundaries  \\\n0             0  http://www.wikidata.org/prop/direct/P31            [4, 109]   \n1             0  http://www.wikidata.org/prop/direct/P31            [4, 109]   \n2             0  http://www.wikidata.org/prop/direct/P31           [71, 109]   \n3             0  http://www.wikidata.org/prop/direct/P47          [167, 184]   \n4             0  http://www.wikidata.org/prop/direct/P47          [167, 191]   \n5             0  http://www.wikidata.org/prop/direct/P47          [167, 224]   \n6             0  http://www.wikidata.org/prop/direct/P47          [167, 184]   \n7             0  http://www.wikidata.org/prop/direct/P47          [167, 191]   \n8             0  http://www.wikidata.org/prop/direct/P47          [186, 224]   \n9             0  http://www.wikidata.org/prop/direct/P47          [167, 224]   \n10            0  http://www.wikidata.org/prop/direct/P47          [186, 224]   \n11            0  http://www.wikidata.org/prop/direct/P47          [167, 184]   \n12            0  http://www.wikidata.org/prop/direct/P47          [167, 191]   \n13            0  http://www.wikidata.org/prop/direct/P47          [167, 224]   \n14            0  http://www.wikidata.org/prop/direct/P47          [167, 184]   \n\n    e1_node  e2_node  \n0         2       17  \n1         2       17  \n2         9       17  \n3        26       28  \n4        26       30  \n5        26       36  \n6        28       26  \n7        30       26  \n8        30       36  \n9        36       26  \n10       36       30  \n11       26       28  \n12       26       30  \n13       26       36  \n14       28       26  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_sentence</th>\n      <th>relation_uri</th>\n      <th>relation_boundaries</th>\n      <th>e1_node</th>\n      <th>e2_node</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P31</td>\n      <td>[4, 109]</td>\n      <td>2</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P31</td>\n      <td>[4, 109]</td>\n      <td>2</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P31</td>\n      <td>[71, 109]</td>\n      <td>9</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 184]</td>\n      <td>26</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 191]</td>\n      <td>26</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 224]</td>\n      <td>26</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 184]</td>\n      <td>28</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 191]</td>\n      <td>30</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[186, 224]</td>\n      <td>30</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 224]</td>\n      <td>36</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[186, 224]</td>\n      <td>36</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 184]</td>\n      <td>26</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 191]</td>\n      <td>26</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 224]</td>\n      <td>26</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0</td>\n      <td>http://www.wikidata.org/prop/direct/P47</td>\n      <td>[167, 184]</td>\n      <td>28</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo remove duplicates from the dataframe\n",
    "\n",
    "df_rel.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "64e96b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'max_num_of_nodes': 333,\n 'num_of_graph_relations': 45,\n 'num_of_dimension': 300,\n 'num_of_relations_to_predict': 78073}"
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations = [x for x in df_rel['relation_uri']]\n",
    "\n",
    "args = {\n",
    "    \"max_num_of_nodes\" : max([len(g.nodes) for g in df_sent['sentence_graph']]),\n",
    "    \"num_of_graph_relations\" : len(nlp_en.get_pipe(\"parser\").labels),\n",
    "    \"num_of_dimension\" : 300,\n",
    "    \"num_of_relations_to_predict\": len(relations)\n",
    "}\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99ec9ba6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P31', 'annotator': 'NoSubject-Triple-aligner'}, 'object': {'boundaries': [94, 109], 'surfaceform': 'language family', 'uri': 'http://www.wikidata.org/entity/Q25295', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [4, 27], 'surfaceform': 'Austroasiatic languages', 'uri': 'http://www.wikidata.org/entity/Q33199', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'NoSubject-Triple-aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P31', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [94, 109], 'surfaceform': 'language family', 'uri': 'http://www.wikidata.org/entity/Q25295', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [4, 27], 'surfaceform': 'Austroasiatic languages', 'uri': 'http://www.wikidata.org/entity/Q33199', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P31', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [94, 109], 'surfaceform': 'language family', 'uri': 'http://www.wikidata.org/entity/Q25295', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [71, 80], 'surfaceform': 'Mon–Khmer', 'uri': 'http://www.wikidata.org/entity/Q33199', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [174, 184], 'surfaceform': 'Bangladesh', 'uri': 'http://www.wikidata.org/entity/Q902', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [174, 184], 'surfaceform': 'Bangladesh', 'uri': 'http://www.wikidata.org/entity/Q902', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [0, 225], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [174, 184], 'surfaceform': 'Bangladesh', 'uri': 'http://www.wikidata.org/entity/Q902', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [174, 184], 'surfaceform': 'Bangladesh', 'uri': 'http://www.wikidata.org/entity/Q902', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [167, 172], 'surfaceform': 'India', 'uri': 'http://www.wikidata.org/entity/Q668', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 0, 'predicate': {'boundaries': [209, 215], 'surfaceform': 'border', 'uri': 'http://www.wikidata.org/prop/direct/P47', 'annotator': 'Wikidata_Property_Linker'}, 'object': {'boundaries': [186, 191], 'surfaceform': 'Nepal', 'uri': 'http://www.wikidata.org/entity/Q837', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [219, 224], 'surfaceform': 'China', 'uri': 'http://www.wikidata.org/entity/Q148', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'SPOAligner'}\n",
      "{'sentence_id': 1, 'predicate': {'boundaries': [226, 319], 'surfaceform': None, 'uri': 'http://www.wikidata.org/prop/direct/P361', 'annotator': 'Simple-Aligner'}, 'object': {'boundaries': [293, 297], 'surfaceform': 'Asia', 'uri': 'http://www.wikidata.org/entity/Q48', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'dependency_path': None, 'confidence': None, 'subject': {'boundaries': [307, 317], 'surfaceform': 'South Asia', 'uri': 'http://www.wikidata.org/entity/Q771405', 'annotator': 'Wikidata_Spotlight_Entity_Linker'}, 'annotator': 'Simple-Aligner'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "307",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(triple)\n\u001B[1;32m     67\u001B[0m subj_boundaries \u001B[38;5;241m=\u001B[39m triple[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboundaries\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 68\u001B[0m subj_ids \u001B[38;5;241m=\u001B[39m \u001B[43mget_idx\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubj_string_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubj_boundaries\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_offsets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m obj_uri \u001B[38;5;241m=\u001B[39m triple[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muri\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     71\u001B[0m obj_string \u001B[38;5;241m=\u001B[39m triple[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurfaceform\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36mget_idx\u001B[0;34m(string_list, boundaries, token_offsets)\u001B[0m\n\u001B[1;32m     61\u001B[0m     len_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(string_list[r:]))\n\u001B[1;32m     62\u001B[0m     offset \u001B[38;5;241m=\u001B[39m boundaries[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m-\u001B[39mlen_string\n\u001B[0;32m---> 63\u001B[0m     ids\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtoken_offsets\u001B[49m\u001B[43m[\u001B[49m\u001B[43moffset\u001B[49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "\u001B[0;31mKeyError\u001B[0m: 307"
     ]
    }
   ],
   "source": [
    "############## This cell should be ignored\n",
    "\n",
    "dict_sentences = {}\n",
    "graphs = []\n",
    "full_rels = []\n",
    "\n",
    "df_sent_columns = ['id_sentence', 'sentence_graph', 'sentence_string']\n",
    "df_rel_columns = ['id_sentence', 'relation', 'e1_node', 'e2_node', 'boundaries']\n",
    "\n",
    "df_sent = []\n",
    "df_rel = []\n",
    "\n",
    "for enum_sent, sentence in enumerate(data_sentences):\n",
    "    g = nx.Graph()\n",
    "    dict_embeddings = {}\n",
    "    edge_list = []\n",
    "    skipped_tokens = 0\n",
    "    relations_list = []\n",
    "\n",
    "    sentence_string = sentence['sentence']\n",
    "    triples = sentence['triples']\n",
    "    token_offsets = {}\n",
    "    \n",
    "    sent_embeddings = get_words_vector(sentence_string, tokenizer, model, layers)\n",
    "    doc_spacy = nlp_en(sentence_string)\n",
    "    \n",
    "    for token in doc_spacy:\n",
    "        token_offsets[token.idx] = (token.i, token.text)\n",
    "        token_idx = tokenizer.encode(token.text, add_special_tokens=False)\n",
    "        \n",
    "        token_embeddings = []\n",
    "        for enum_idx, token_id in enumerate(token_idx):\n",
    "            token_embeddings.append(sent_embeddings[token.i+enum_idx+skipped_tokens])\n",
    "            \n",
    "        skipped_tokens += enum_idx\n",
    "\n",
    "            \n",
    "        if len(token_embeddings) > 1:\n",
    "            token_embeddings = torch.stack(token_embeddings).to(device)\n",
    "            token_embeddings = torch.mean(token_embeddings, -2)\n",
    "\n",
    "        else:\n",
    "            token_embeddings = token_embeddings[0]\n",
    "\n",
    "        token_embeddings = torch.reshape(token_embeddings, (1,768))\n",
    "\n",
    "        dict_embeddings[token.i] = token_embeddings\n",
    "        \n",
    "        start_token = token.idx\n",
    "        end_token = token.idx + len(token.text)\n",
    "        \n",
    "        g.add_node(token.i, label=token.text, type='token', features=token_embeddings, boundaries=(start_token, end_token))\n",
    "        \n",
    "        edge_list.append((token.i, token.head.i, token.dep_))\n",
    "        \n",
    "    row_sent = [enum_sent, g, sentence_string]\n",
    "    df_sent.append(row_sent)\n",
    "    \n",
    "    \n",
    "    next_i = token.i + 1\n",
    "    \n",
    "    for triple in triples:\n",
    "        subj_uri = triple['subject']['uri']\n",
    "        subj_string = triple['subject']['surfaceform']\n",
    "        subj_string_list = subj_string.split()\n",
    "        print(triple)\n",
    "        subj_boundaries = triple['subject']['boundaries']\n",
    "        subj_ids = get_idx(subj_string_list, subj_boundaries, token_offsets)\n",
    "\n",
    "        obj_uri = triple['object']['uri']\n",
    "        obj_string = triple['object']['surfaceform']\n",
    "        obj_string_list = obj_string.split()\n",
    "        obj_boundaries = triple['object']['boundaries']\n",
    "        obj_ids = get_idx(obj_string_list, obj_boundaries, token_offsets)\n",
    "\n",
    "        #embeddings_subj = []\n",
    "        #embeddings_obj = []\n",
    "\n",
    "        subj_i = int(next_i)\n",
    "        obj_i = int(next_i+1)\n",
    "        #g.add_node(subj_i, label=subj_string, type='entity')\n",
    "        \n",
    "        #for i in subj_ids:\n",
    "            #embeddings_subj.append(dict_embeddings[i])\n",
    "            #edge_list.append((i, subj_i))\n",
    "\n",
    "        #g.add_node(obj_i, label=obj_string, type='entity')\n",
    "        #for i in obj_ids:\n",
    "        #    embeddings_obj.append(dict_embeddings[i])\n",
    "        #    edge_list.append((i, obj_i))\n",
    "\n",
    "        #if len(embeddings_subj) > 1:\n",
    "        #    embeddings_subj = torch.stack(embeddings_subj).to(device)\n",
    "        #    embeddings_subj = torch.mean(embeddings_subj, -2)\n",
    "        #else:\n",
    "        #    embeddings_subj = torch.stack(embeddings_subj).to(device)\n",
    "\n",
    "\n",
    "        #if len(embeddings_obj) > 1:\n",
    "        #    embeddings_obj = torch.stack(embeddings_obj).to(device)\n",
    "        #    embeddings_obj = torch.mean(embeddings_obj, -2)\n",
    "        #else:\n",
    "        #    embeddings_obj = torch.stack(embeddings_obj).to(device) \n",
    "\n",
    "\n",
    "        #dict_embeddings[subj_i] = embeddings_subj\n",
    "        #dict_embeddings[obj_i] = embeddings_obj\n",
    "\n",
    "        #next_i += 2\n",
    "\n",
    "        ##-----these might be useful in future------\n",
    "        rel_boundaries = triple['predicate']['boundaries']\n",
    "        rel_surfaceform = triple['predicate']['surfaceform']\n",
    "        rel_uri = triple['predicate']['uri']\n",
    "        relations_list.append((subj_i, obj_i, rel_uri))\n",
    "\n",
    "\n",
    "        text = data[0][triple['sentence_id']]['text']\n",
    "        ##----------------------------------------------\n",
    "        \n",
    "        row_rel = [enum_sent,subj_ids, obj_ids, rel_boundaries]\n",
    "        df_rel.append(row_rel)\n",
    "\n",
    "        #predicate_string = data[0][0]['text'][min(obj_boundaries[0], subj_boundaries[0]):max(obj_boundaries[1],subj_boundaries[1])] if rel_boundaries == None else data[0][0]['text'][rel_boundaries[0]:rel_boundaries[1]]    \n",
    "\n",
    "    #g.add_edges_from(edge_list)\n",
    "    #graphs.append(g)\n",
    "    #full_rels.append(relations_list)\n",
    "    #dict_sentence[enum_doc] = dict_embeddings\n",
    "\n",
    "df_rel = pd.DataFrame(df_rel, columns=df_rel_columns)\n",
    "df_sent = pd.DataFrame(df_sent, columns = df_sent_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "82c8767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GAE, RGCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "outputs": [],
   "source": [
    "class RGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, feature_channels, hidden_channels, num_relations):\n",
    "        super().__init__()\n",
    "        # todo try FastRGCNConv as well, as we have GPUs with each 48GB VRAM\n",
    "        self.conv1 = RGCNConv(feature_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=8)\n",
    "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=5)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        x = self.conv1(x, edge_index, edge_type).relu_()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, num_relations, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden_channels * 2, num_relations)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        z_src, z_dst = z[edge_index[0]], z[edge_index[1]]\n",
    "        concat = torch.cat((z_src, z_dst), 1)\n",
    "        x = self.linear(concat)\n",
    "        return torch.nn.ReLU()(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\n",
      "tensor(115.0607, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(114.5007, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(111.4412, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(111.2930, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(113.0909, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(110.5646, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(110.3330, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(110.1276, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(107.6038, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(102.7915, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(108.5110, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(107.8984, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(105.2052, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(103.2536, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(106.4586, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(110.9473, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(103.0833, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(100.9531, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.4009, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(97.5503, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(96.2559, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(101.1249, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(101.2919, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(93.2832, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(92.9472, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(79.7402, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(93.5830, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.5263, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.1224, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(104.4355, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(110.4086, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(107.3059, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.5479, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(110.6290, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.6125, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(105.1392, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(104.1235, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(87.9894, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(72.2745, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(88.9353, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(72.4486, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.6794, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.5708, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.9594, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.7347, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.8860, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.5294, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(68.0392, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(60.9137, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.9979, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.6107, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(55.6603, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.5260, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(56.9297, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.6354, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.3487, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.4371, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.0494, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.9858, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(58.9756, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.7981, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.2235, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.9801, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.0048, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(73.6624, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(88.7734, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.2072, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(86.4554, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(82.5315, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.4562, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(59.0915, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(69.4111, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(99.9752, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(99.7518, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.8997, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.2190, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.7016, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(90.3032, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(104.5210, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(100.1283, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(72.3445, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.8549, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.3145, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.3192, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(109.8035, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(68.9857, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(97.7653, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.9641, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(68.4407, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(95.8999, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(106.0708, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(56.8226, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.2616, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.2581, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.0996, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(88.2406, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.0108, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.2385, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(97.9812, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.3454, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.7108, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.6660, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.6079, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(92.1054, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(90.3910, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(97.2980, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.2785, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(69.8448, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.3370, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(116.8673, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(92.4364, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.1549, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(56.0584, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.5039, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(67.7184, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(60.0642, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.1370, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(42.3238, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(63.7698, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.1985, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.8894, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(46.1379, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.7793, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(110.9738, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(91.2799, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(104.0895, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.9781, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(67.6149, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(41.9845, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(90.5738, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.6310, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(52.0983, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(58.7364, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.4257, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(92.9634, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(59.5211, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.6866, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.4412, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.3794, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(100.5749, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(86.5760, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.7095, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(107.8811, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(53.2096, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(82.1666, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(97.2033, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.5875, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(83.4726, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.0906, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(48.5335, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.1245, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(88.6313, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(87.9713, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(97.8689, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.8589, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.4305, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.5031, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.0994, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.8398, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(53.2145, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(83.3945, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(98.9674, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.0990, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.6811, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.8527, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.8482, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.5349, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(86.1952, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.1154, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(92.1809, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.6955, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(55.8193, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(105.9117, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.1288, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(86.5833, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(59.4530, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(86.8974, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(104.2360, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.9006, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(67.6287, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(45.4775, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.1853, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(53.8036, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.1127, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(107.9275, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(48.4448, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(73.1833, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(62.2165, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(41.2740, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.7882, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(56.9466, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.4653, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.5297, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.0623, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(98.5467, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.0166, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.1495, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.8053, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(60.1550, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(79.9020, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.1142, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(90.6811, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.2732, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(88.8529, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.9180, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(93.1853, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.9016, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(62.6649, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.0540, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.0891, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(63.9365, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.3738, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(56.2642, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(82.4111, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.0442, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.1878, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(82.1609, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(41.5838, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(58.7737, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(58.9376, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.6448, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(69.2194, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.4747, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(43.4381, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.0116, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.5181, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.9484, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(48.2373, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(71.3966, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(55.3517, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(62.0006, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(93.8991, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(90.3139, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.2523, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.5914, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(86.2711, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(68.5337, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(27.5679, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(48.4140, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.4270, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.0552, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.6358, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(56.5978, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(50.1597, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(83.6678, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.8762, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(71.4024, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(88.4672, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(69.1548, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(91.5979, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(87.5389, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.1218, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(90.1171, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(91.2754, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(40.4585, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.6931, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.0027, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.6212, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(54.9648, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(52.4338, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(56.7273, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.2332, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(38.6047, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.2147, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(73.8815, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.2247, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(50.5343, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(39.8022, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(43.9323, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(22.7229, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(43.5926, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(62.3761, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(43.6201, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(52.7796, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(86.4211, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(93.5056, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.7918, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(73.2104, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(36.6095, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(57.1373, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(42.0459, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(63.3960, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.7753, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(63.6699, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(103.6087, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.6605, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.4113, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(102.1784, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.3053, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.0712, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.0490, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(102.3841, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(99.9696, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(69.2641, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(87.3484, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(98.6869, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(46.1658, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(34.5396, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(49.4068, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.5118, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.0424, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(59.4309, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(83.7984, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(73.8266, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.3596, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.4872, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(59.6606, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(46.3941, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(48.6939, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(49.5182, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(52.1354, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.0430, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(105.8021, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(40.9122, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.8619, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.7022, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.6298, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(87.8807, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.2224, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(45.5450, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(53.5684, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(45.5396, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.2038, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.3016, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.3486, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.0656, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.1451, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(37.3037, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(79.8107, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(63.6446, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(54.4390, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.2484, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(63.2997, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(115.1181, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(108.8164, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(94.7659, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.3029, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(68.3356, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(96.1198, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.5767, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.0815, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.2716, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.3092, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(86.0716, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(102.2788, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.3828, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.1658, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(93.6718, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(100.1602, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(83.2302, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(89.6566, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(96.5987, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.9536, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(79.9654, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(83.2401, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(87.3978, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.8199, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.4118, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(95.9883, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.7973, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(54.7098, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.2526, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.0683, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.6144, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.5724, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(55.3718, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.7773, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.8499, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.5026, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(93.7912, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(55.5226, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(87.2007, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(73.4419, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(57.0314, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(69.9491, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(93.3263, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(49.3552, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(63.5271, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(104.0660, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(73.0715, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(87.1475, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.9684, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(68.5525, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(45.4751, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(72.1758, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.0860, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(68.4999, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(99.0911, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(96.3827, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(57.5650, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(83.3839, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.4984, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.0957, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.1214, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(43.4057, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.9562, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(59.2258, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.9876, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(100.2081, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(56.2293, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(82.5545, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(76.9463, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(63.7370, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(111.8707, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(73.3766, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.1683, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.3472, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.3467, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.5815, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(62.5845, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(62.4590, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.9881, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(79.3751, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.6633, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.6697, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(90.0509, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(68.6356, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(99.4381, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.3978, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(95.6645, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(78.6638, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.0395, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(57.4883, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.5760, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.1187, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(77.3253, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(67.0973, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.2317, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(74.4599, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.7426, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.9506, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(65.9830, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(90.6745, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(66.1959, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(35.6448, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(81.0796, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(84.6282, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(73.2825, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(48.9003, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(35.9534, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(32.9730, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(48.4941, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(37.1284, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(26.0797, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(85.5479, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(62.7054, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(22.1525, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(53.3873, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(34.6865, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(47.2892, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(48.4529, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(43.6771, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(45.6648, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(45.9326, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.7435, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(53.2098, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(37.0598, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(48.8824, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(80.4363, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(42.1074, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(51.8706, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(68.7517, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(75.6605, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(61.6694, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(52.8778, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(58.7305, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(54.9233, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(54.5757, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(64.0240, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(50.3205, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(39.2385, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(49.5752, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(70.2489, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(43.3163, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(67.2830, grad_fn=<AddBackward0>)\n",
      "batch\n",
      "tensor(53.7586, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "model = GAE(\n",
    "    RGCNEncoder(feature_channels=768,\n",
    "                hidden_channels=200,\n",
    "                num_relations=args[\"num_of_graph_relations\"]),\n",
    "    Decoder(args[\"num_of_relations_to_predict\"],\n",
    "            hidden_channels=200),\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "graphs = []\n",
    "for idx, sentence_graph in df_sent[['id_sentence', 'sentence_graph']].iterrows():\n",
    "\n",
    "    g = Data(x=torch.stack([sentence_graph['sentence_graph'].nodes[id_node]['features'].flatten(0) for id_node in sentence_graph['sentence_graph'].nodes]),\n",
    "             edge_index=torch.tensor(list(sentence_graph['sentence_graph'].edges())).T,\n",
    "             y=sentence_graph['id_sentence'],\n",
    "             edge_type=torch.tensor([nlp_en.get_pipe(\"parser\").labels.index(sentence_graph['sentence_graph'].get_edge_data(x,y)['label']) for (x,y) in sentence_graph['sentence_graph'].edges()]))\n",
    "    graphs.append(g)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(graphs, batch_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "# todo how to parallelize?\n",
    "for epoch in range(0, 100):\n",
    "    for batch in dataloader:\n",
    "        cross_entropy_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # todo implement batch processing for the RGCN that does not need the number of edges in advance (use pty to construct the new node id that is used in the batch)\n",
    "        for i in range(batch_size):\n",
    "            sample = batch.get_example(i)\n",
    "            id_sentence = sample.y.item()\n",
    "\n",
    "            e1_nodes = torch.tensor(df_rel.loc[df_rel['id_sentence'] == id_sentence]['e1_node'].values)\n",
    "            e2_nodes = torch.tensor(df_rel.loc[df_rel['id_sentence'] == id_sentence]['e2_node'].values)\n",
    "            edge_index_relations = torch.stack([e1_nodes, e2_nodes])\n",
    "            y_true = torch.tensor([relations.index(r) for r in df_rel.loc[df_rel['id_sentence'] == id_sentence]['relation_uri'].values])\n",
    "\n",
    "            if y_true.size(0) > 0:\n",
    "                z = model.encode(sample.x, sample.edge_index, sample.edge_type)\n",
    "                y_pred = model.decode(z, edge_index_relations)\n",
    "                l = loss_function(y_pred, y_true)\n",
    "                cross_entropy_loss += l\n",
    "\n",
    "        print(cross_entropy_loss)\n",
    "        if torch.is_tensor(cross_entropy_loss) and cross_entropy_loss.requires_grad:\n",
    "            cross_entropy_loss.backward()\n",
    "            optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}